---
layout: post
title: "DeepFool笔记"
date: 2019-12-16 16:04:06 
description: "DeepFool笔记"
tag: Adversarial Examples
---

`入门对抗样本有一个月的时间了，这一个月看了不少篇论文，也有不少水文，所以博客更新的不是很频繁，最近看论文发现前面还有不少classical的方法不是很清楚，于是翻回去看一看，这篇文章是CVPR2016的，想法很好，是洛桑联邦理工学院的研究人员写的，可能是因为我的英语水平有的菜，感觉文章写的高深莫测，实际上理解了也就那些东西，但是角度还是非常novel的，这篇文章和MIT在NIPS19上的工作都很好，引用也非常多，值得我们借鉴和学习。`

### Abstract

作者首先笼统的介绍一下background，然后说当前没有一个精确攻击的模型，我们提出了一个方法来解决这个问题。`什么是精确攻击，试想经典的FGSM的中，更新的操作是求一个sign()，那直接的，所有变化都没有区别了,一定程度上是L1正则化，可是在正则化的同时也失去的精度（个人理解，可能有错）`那也就是说作者是为了解决这个问题，让攻击更加精准，作者用了一个例子的图很清楚的展示了这个问题。

![DeepFool_FGSM_comparation](/images/posts/DeepFool_FGSM_comparation.png)

`这里疯狂吐槽一个点，一些对抗样本的文章实际上是有不少改动的，并不是一个肉眼看不出来的变化，但是论文里面放小图，又不开源代码，这样根本看不出来有什么变化，但是一放大图，直接露馅，这篇文章敢开源和放大图，工作做的确实很solid。`

### Introduction

`本文有大量的公式，我的博客对于markdown的公式解析不好，所以我这里只标记公式序号，建议对照原文阅读笔记。`

作者定义了一个最小扰动，见公式（1），简单的说就是这个最小的扰动需要刚刚好让分类器类别改变，刚刚超过分类器的判别边界。作者同时定义了一个分类器的鲁棒系数，见公式（2），是数据集上扰动的临界值除以x的L2距离，讲道理我觉得这里对于不同模型来说只有分子会变动，分母是不变的，但是看网上一些教程说分母也会变，所以不是很理解，感觉我的理解应该是对的。

作者总结自己的工作：

- 提出了一种简单但是精确的方法来比较不同模型对于对抗样本的鲁棒性

- 大量实验表面，我们的方法计算对抗扰动更可靠和有效，对抗训练有助于提高模型鲁棒性

- 我们的方法更好的解释了对抗样本和其扰动现象。

related work这里就忽略了哈～

### DeepFool for binary classifiers

作者首先研究一个俩分类问题，然后拓展到多分类问题。

- 俩分类问题
  来，我们回到高中做平面几何，判断$x_0$在直线的左边还是右边？
  ![DeepFool_binary_classifier](/images/posts/DeepFool_binary_classifier.png)
  一看是在右边。
  第一个问题那怎么样扰动到左边最近呢？当然是垂直啊！
  那也就是说如果对于二分类问题，当边界是f(x) = ax + b 的这种直线的时候，实际上对抗样本的最小扰动就是垂直于该条直线的方向。
  
  ---
  
  那第二个问题，那我扰动多少呢？当然是x0到线的直线距离多一丢丢啦，要不然怎么变成另一个类别呢～
  所以看而分类问题的更新算法实际上是这样的(如果知乎或者博客显示不出来可以对照markdown语法看看)：

  $-\cfrac{f(x_i)}{||w||_2} * \cfrac{w}{||w||_2}$

  第一个分式实际上是点到直线的距离公式，第二个分式实际上是参数的单位向量，确定的是更新的方向。当时乍一看论文里的公式不是很好明白，拆开以后发现没有那么深奥。

  对了前面说到要多一丢丢，实际上作者做的时候做了如下操作：
  > In order to reach the other side of the classification boundary, the final perturbation vector rˆis multip lied by a constant 1+η,with η ≪ 1. In our experiments, we have used η = 0.02.

### DeepFool for multiclass classifiers

二分类搞定了，现在开始搞多分类。
多分类实际上是多个二分类问题（讲道理也不一样），那它的分类边界是一个很抽象的高维空间，所以并不是很好想象，那么我们简单来看，一个很复杂的高维度空间中不同的区域代表不同的类别，那么很简单，我如果想扰动最小，我就需要找到最近的类别，这也就是公式（6）说的存在的含义，存在一个k，也就是说只要有一个k，使得第k个分类器输出的值大于样本本身的类别输出的值即可。
举个栗子，我有猫，狗，和人三类，我输入一个人的样本，如果要进行攻击，那么我需要人这个样本过分类器的时候softmax输出的值在狗这个类别上大于人这个类别即可。
那么理解这个，公式（7）也就理解了，所有对特定k的x0的区域的交集就是这个k的区域。
那实际上（8）和（9）的公式就是二分类问题的拓展，（8）就是定方向，（9）就是跟上述公式一样的，只不过是多维而已，而且说是多维，实际上也只针对了俩个区域。

---

画个重点，这个算法是可以迭代的，迭代的需要用到泰勒展开的一些知识，我有点忘记了，但是找到了一篇中科大小哥的博客，说的很好，在此分享链接：
https://tanjuntao.github.io/2019/07/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ADeepFool-a-simple-and-accurate-method-to-fool-neural-networks/

---

后面有一些关于模型鲁棒性的实验，讲道理也是比较好的，但是今天状态不是很好，看不下去了，告辞，改天再说。
